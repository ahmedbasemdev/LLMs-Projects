## Fine Tunning LLama 2 for Text Summarization

Fine tunning large language model "llama 2" using PEFT and LORA for text summarization in CNN Daily Mail dataset

### LORA
LoRA decomposes a weight matrix into two smaller weight matrices, as illustrated below, to approximate full supervised fine-tuning in a more parameter-efficient manner.
![LORA](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage7.png)

* Dataset:
https://huggingface.co/datasets/cnn_dailymail


